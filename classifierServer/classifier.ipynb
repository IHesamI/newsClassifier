{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150096 entries, 0 to 150095\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  150096 non-null  int64 \n",
      " 1   Text        150096 non-null  object\n",
      " 2   Category    150096 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df= pd.read_csv('train.csv')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('rawcategories.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149650</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149658</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149718</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149733</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150022</th>\n",
       "      <td>Natural Environment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "558     Natural Environment\n",
       "805     Natural Environment\n",
       "1039    Natural Environment\n",
       "1249    Natural Environment\n",
       "1350    Natural Environment\n",
       "...                     ...\n",
       "149650  Natural Environment\n",
       "149658  Natural Environment\n",
       "149718  Natural Environment\n",
       "149733  Natural Environment\n",
       "150022  Natural Environment\n",
       "\n",
       "[851 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[0]=='Natural Environment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cluster_mapping = {\n",
    "    \"economic\": [\n",
    "        'Economy',\n",
    "        'Economy.Bank and Bourse',\n",
    "        'Economy.Oil',\n",
    "        'Economy.Commerce',\n",
    "        'Economy.Industry',\n",
    "        'Economy.Agriculture',\n",
    "        'Economy.Dwelling and Construction'\n",
    "    ],\n",
    "    \"cultural and artistic\": [\n",
    "        'Science and Culture',\n",
    "        'Science and Culture.Science.Book',\n",
    "        'Literature and Art',\n",
    "        'Literature and Art.Art',\n",
    "        'Literature and Art.Art.Cinema',\n",
    "        'Literature and Art.Art.Music',\n",
    "        'Literature and Art.Art.Theater',\n",
    "        'Science and Culture.Science',\n",
    "        'Science and Culture.Science.Medicine and Remedy',\n",
    "        'Literature and Art.Literature'\n",
    "    ],\n",
    "    \"political\": [\n",
    "        'Politics',\n",
    "        'Politics.Iran Politics'\n",
    "    ],\n",
    "    \"technological\": [\n",
    "        'Science and Culture.Science.Information and Communication Technology'\n",
    "    ],\n",
    "    \"sports\": [\n",
    "        'Sport',\n",
    "        'Sport.World Cup'\n",
    "    ],\n",
    "    \"social\": [\n",
    "        'Social',\n",
    "        'Social.Women',\n",
    "        'Social.Religion'\n",
    "    ],\n",
    "    \"miscellaneous\": [\n",
    "        'Miscellaneous',\n",
    "        'Miscellaneous.World News',\n",
    "        'Miscellaneous.Urban',\n",
    "        'Miscellaneous.Picture',\n",
    "        'Miscellaneous.Happenings',\n",
    "        'Miscellaneous.Islamic Councils',\n",
    "        'Miscellaneous.Picture.Caricature'\n",
    "    ],\n",
    "    \"tourism\": [\n",
    "        'Tourism'\n",
    "    ],\n",
    "    \"environment\": [\n",
    "        'Natural Environment'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def categoesMapping(cat):\n",
    "    for key,value in topic_cluster_mapping.items():\n",
    "        if cat in value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "categiesdData=train_df['Category'].apply(categoesMapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150096,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Category'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 1, ..., 3, 0, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows  \n",
    "# how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "categiesdData= label_encoder.fit_transform(categiesdData) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(categiesdData, columns=['Category'],index=None).to_csv('categoriesData.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['econom',\n",
       " 'cultural and artist',\n",
       " 'polit',\n",
       " 'technolog',\n",
       " 'sport',\n",
       " 'social',\n",
       " 'miscellan',\n",
       " 'tourism',\n",
       " 'natural environ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemList=[]\n",
    "for x in list(category_keywords.keys()):\n",
    "    stemList.append(ps.stem(x))\n",
    "stemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "X = train_df['Category']\n",
    "documents = []\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import difflib\n",
    "def categories (category):\n",
    "    # regex1= r'cultur(e|al)'\n",
    "    # regex2 = r'(art)(ist)*'\n",
    "\n",
    "    result=difflib.get_close_matches(category,candidate_labels)\n",
    "    if len(result)>0:\n",
    "        return result[0]\n",
    "    return 'miscellaneous'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Science and Culture\n",
       "1                        Sport\n",
       "2                      Economy\n",
       "3     Miscellaneous.World News\n",
       "4                        Sport\n",
       "5          Miscellaneous.Urban\n",
       "6                        Sport\n",
       "7                 Social.Women\n",
       "8                       Social\n",
       "9     Miscellaneous.World News\n",
       "10          Literature and Art\n",
       "11                    Politics\n",
       "12         Science and Culture\n",
       "13               Miscellaneous\n",
       "14                      Social\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Category'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoriesdData = train_df['Category'].apply(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 1, ..., 3, 0, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categiesdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('classifiedCategories.csv',header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=train_df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , x_test ,y_train ,y_test = train_test_split(train_df['Text'],categiesdData,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "def preProcessText(x:str):\n",
    "    x = remove_urls(x)\n",
    "    x = re.sub(r'[^\\w\\s]','',x)\n",
    "    x = re.sub(r'\\d', '', x)\n",
    "    \n",
    "    # remove all single characters\n",
    "    x = re.sub(r'\\s+[a-zA-Z]\\s+', '', x)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    x = re.sub(r'\\^[a-zA-Z]\\s+', '', x) \n",
    "\n",
    "    x = re.sub(r'\\n', '', x) \n",
    "    \n",
    "    stopWordsFile = open('./stopwords.dat')\n",
    "    stopWordsFile2 = open('./persian-stopwords/persian')\n",
    "    combined=stopWordsFile.read().split('\\n') + stopWordsFile2.read().split('\\n')\n",
    "    stopWords= set(combined)\n",
    "    if 'برای' in stopWords:\n",
    "        print('wtf')\n",
    "    splitedText = [word for word in x.split(' ') ]\n",
    "    print(splitedText)\n",
    "    result=[]\n",
    "    for word in splitedText:\n",
    "        if word not in stopWords:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtf\n",
      "['نمايش', 'ويژه', 'زماني', 'براي', 'مستي', 'اسبها', 'فيلم', 'زماني', 'براي', 'مستي', 'اسبها', 'براي', 'روزنامه', 'نگاران', 'و', 'خانواده', 'هاي', 'آنان', 'نمايش', 'داده', 'مي', 'شود', 'اين', 'فيلم', 'دوشنبه', '', 'دي', '', 'ساعت', 'در', 'سينما', 'سپيده', 'براي', 'اعضاي', 'انجمن', 'صنفي', 'روزنامه', 'نگاران', 'و', 'خانواده', 'آنان', 'به', 'نمايش', 'درمي', 'آيد', 'اعضاي', 'متقاضي', 'مي', 'توانند', 'جهت', 'رزرو', 'بليت', 'با', 'شماره', 'هاي', '', '', '', 'و', 'تماس', 'حاصل', 'كنند', 'گفتني', 'است', 'فيلم', 'زماني', 'براي', 'مستي', 'اسبها', 'به', 'كارگرداني', 'بهمن', 'قبادي', 'برنده', 'دوربين', 'طلايي', 'جشنواره', 'كن', 'امسال', 'بوده', 'است', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'نمايش ويژه زماني براي مستي اسبها فيلم زماني براي مستي اسبها براي روزنامه نگاران خانواده هاي نمايش مي اين فيلم دوشنبه دي ساعت سينما سپيده براي اعضاي انجمن صنفي روزنامه نگاران خانواده نمايش درمي آيد اعضاي متقاضي مي رزرو بليت شماره هاي تماس حاصل كنند گفتني فيلم زماني براي مستي اسبها كارگرداني بهمن قبادي برنده دوربين طلايي جشنواره كن'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessText(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import word_tokenize\n",
    "# converting each sentence to list of words and inserting in sents\n",
    "X_train = X_train.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [خبرنامه, دانشگاه, علم, صنعت, ايران, شماره, يا...\n",
       "1       [پايان, سال, دهها, زمين, فوتبال, سالن, ورزش, ب...\n",
       "2       [انجمن, توليدكنندگان, تجهيزات, صنعت, نفت, تشكي...\n",
       "3       [كرتين, براي, سومين, وزير, كانادا, ژان, كرتين,...\n",
       "4       [رفقا, نمايندگان, اروپاي, شرقي, جام, بابك, كما...\n",
       "                              ...                        \n",
       "9995    [مسئوليت, شهروندان, رئيس, جمهور, راي, خويش, تي...\n",
       "9996    [مديرمسئول, روزنامه, همبستگي, دادگاه, احضار, گ...\n",
       "9997    [شيرهاي, آب, توجه, موقعيت, بحراني, ذخاير, آب, ...\n",
       "9998    [نيم, رخ, نگاهي, زندگي, جبران, خليل, جبران, پس...\n",
       "9999    [پيروزي, پيكان, مدعي, واليبال, برتر, باشگاههاي...\n",
       "Name: Text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=50, window=5,seed=42, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ليگ', 0.9693446755409241),\n",
       " ('باشگاهي', 0.9688852429389954),\n",
       " ('بازيهاي', 0.9644466042518616),\n",
       " ('قهرماني', 0.9510931372642517),\n",
       " ('تيمهاي', 0.9510273337364197),\n",
       " ('تيمي', 0.9507378339767456),\n",
       " ('رقابتهاي', 0.9420532584190369),\n",
       " ('برتر', 0.9419983625411987),\n",
       " ('بازيكنانش', 0.9363963007926941),\n",
       " ('گيران', 0.9300103187561035)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('اقتصاد',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
