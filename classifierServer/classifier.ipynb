{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import saveObject_ , categoesMapping ,loadObject_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "categiesdData=train_df['Category'].apply(categoesMapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         cultural and artistic\n",
       "1                        sports\n",
       "2                      economic\n",
       "3                 miscellaneous\n",
       "4                        sports\n",
       "                  ...          \n",
       "150091            miscellaneous\n",
       "150092            miscellaneous\n",
       "150093            miscellaneous\n",
       "150094    cultural and artistic\n",
       "150095                political\n",
       "Name: Category, Length: 150096, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categiesdData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "# label_encoder object knows  \n",
    "# how to understand word labels. \n",
    "label_encoder = LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "# E:\\term7\\ML\\FinalProject\\classifierServer\\rawcategories.csv\n",
    "rawCats = pd.read_csv('./rawcategories.csv',header=None)\n",
    "categiesdData = rawCats[0].apply(categoesMapping)\n",
    "categiesdData= label_encoder.fit_transform(categiesdData) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject_(label_encoder,'labelEncoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "        url_pattern = re.compile(r'https?://\\S+')\n",
    "        return url_pattern.sub('', text)\n",
    "    \n",
    "train_df['Text'] = train_df['Text'].apply(remove_urls)\n",
    "\n",
    "train_df['Text'] = train_df['Text'].replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "\n",
    "train_df['Text'] = train_df['Text'].replace(to_replace=r'\\d', value='', regex=True)\n",
    "\n",
    "train_df['Text'] = train_df['Text'].replace(to_replace=r'\\n', value='', regex=True)\n",
    "\n",
    "processedData=train_df['Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopWordsFile2 = open('./persian-stopwords/persian', encoding='utf-8')\n",
    "combined = stopWordsFile2.read().split('\\n')\n",
    "stopWords = set(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processedData=processedData.apply(lambda x: [word for word in x if word not in stopWords])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(processedData).to_csv('finalTexts.csv',header=None,index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('finalTexts.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['خبرنامه', 'دانشگاه', 'علم', 'صنعت', 'ايران',...\n",
       "1    ['پايان', 'سال', 'دهها', 'زمين', 'فوتبال', 'سا...\n",
       "2    ['انجمن', 'توليدكنندگان', 'تجهيزات', 'صنعت', '...\n",
       "3    ['كرتين', 'براي', 'سومين', 'وزير', 'كانادا', '...\n",
       "4    ['رفقا', 'نمايندگان', 'اروپاي', 'شرقي', 'جام',...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df[0].apply(lambda text : eval(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject_(texts,'textsFile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=loadObject_('textsFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=texts.apply(lambda x: [word for word in x if word not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [خبرنامه, دانشگاه, علم, صنعت, ايران, شماره, يا...\n",
       "1    [پايان, سال, دهها, زمين, فوتبال, سالن, ورزش, ك...\n",
       "2    [انجمن, توليدكنندگان, تجهيزات, صنعت, نفت, تشكي...\n",
       "3    [كرتين, سومين, وزير, كانادا, ژان, كرتين, وزير,...\n",
       "4    [رفقا, نمايندگان, اروپاي, شرقي, جام, بابك, كما...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , x_test ,y_train ,y_test = train_test_split(texts,categiesdData,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VECTOR_SIZE, WINDOW, SEED, MIN_COUNT, WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eniac\\anaconda3\\lib\\site-packages\\requests\\__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils import W2VMODEL\n",
    "w2v_model = Word2Vec(X_train, vector_size=VECTOR_SIZE, window=WINDOW,seed=SEED, min_count=MIN_COUNT, workers=WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject_(w2v_model,W2VMODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    words_vecs = [w2v_model.wv[word] for word in sentence if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "x_test = np.array([vectorize(sentence) for sentence in x_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eniac\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils import LOGISTIC_REGRESSION_MODEL\n",
    "\n",
    "clf = LogisticRegression(class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject_(clf,LOGISTIC_REGRESSION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6869753497668221\n",
      "Precision: 0.6869753497668221\n",
      "Recall: 0.6869753497668221\n",
      "F1 score: 0.7042057688285234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred,average='micro'))\n",
    "print('Recall:', recall_score(y_test, y_pred,average='micro'))\n",
    "print('F1 score:', f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=15,min_samples_leaf=5,class_weight='balanced')\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject_(rfc,'rfclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6869753497668221\n",
      "Precision: 0.7840295195880695\n",
      "Recall: 0.6869753497668221\n",
      "F1 score: 0.7042057688285234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy:', accuracy_score(y_test, rfc_y_pred))\n",
    "print('Precision:', precision_score(y_test, rfc_y_pred,average='weighted'))\n",
    "print('Recall:', recall_score(y_test, rfc_y_pred,average='weighted'))\n",
    "print('F1 score:', f1_score(y_test, rfc_y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['social']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "newText= '''برنامه درسی دو ساله زبان اشاره بریتانیایی شامل دست‌کم ۷۵۰ نشانه است و مخصوص دانش‌آموزانی است که هیچ دانش قبلی از این زبان ندارند. در پایان دوره، دانش‌آموزان باید بتوانند به‌طور موثر با زبان اشاره بریتانیایی ارتباط برقرار کنند.\n",
    "\n",
    "در این دوره دانش‌آموزان همچنین تاریخچه زبان اشاره بریتانیایی، اهمیت زبان‌های اشاره و چگونگی شکل‌گیری این زبان را یاد می‌گیرند.\n",
    "\n",
    "گنجاندن زبان اشاره بریتانیایی در برنامه درسی، یک پیروزی برای افرادی است که سعی داشتند استفاده از این زبان را فراتر از جامعه ناشنوایان گسترش دهند.'''\n",
    "text = re.sub(r'[^\\w\\s]', '', newText)\n",
    "text = re.sub(r'\\d', '', newText)\n",
    "\n",
    "textList=[word for word in text.split(' ') if word]\n",
    "stopWordsFile = open('./stopwords.dat', encoding='utf-8')\n",
    "stopWordsFile2 = open('./persian-stopwords/persian', encoding='utf-8')\n",
    "combined = stopWordsFile.read().split('\\n') + stopWordsFile2.read().split('\\n')\n",
    "stopWords = set(combined)\n",
    "\n",
    "cleanList = [word for word in textList if word not in stopWords]\n",
    "\n",
    "x_test = np.array([vectorize(cleanList)])\n",
    "print(\n",
    "    # label_encoder.inverse_transform(rfc.predict(x_test)),\n",
    "    label_encoder.inverse_transform(clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cultural and artistic', 'economic', 'environment',\n",
       "       'miscellaneous', 'political', 'social', 'sports', 'technological',\n",
       "       'tourism'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LOGISTIC_REGRESSION_MODEL , saveObject\n",
    "saveObject(clf,LOGISTIC_REGRESSION_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
